{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kDeZi1yoZNGS"
      },
      "outputs": [],
      "source": [
        "# Dueling DDQN, PER"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "pip install --upgrade pip && \\\n",
        "pip install tqdm numpy scikit-learn pyglet setuptools && \\\n",
        "pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n",
        "pip install swig\n",
        "pip install git+https://github.com/pybox2d/pybox2d\n",
        "pip install git+https://github.com/mimoralea/gym-bandits\n",
        "pip install git+https://github.com/mimoralea/gym-walk\n",
        "pip install git+https://github.com/mimoralea/gym-aima\n",
        "pip install gym[atari]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFMe9RDVdXN2",
        "outputId": "a971dcf7-c29b-4dfd-85b7-496aa107121b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.10/dist-packages (2.0.20)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (75.1.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: asciinema in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: tornado==5.* in /usr/local/lib/python3.10/dist-packages (5.1.1)\n",
            "Requirement already satisfied: PyBullet in /usr/local/lib/python3.10/dist-packages (3.2.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.3.0)\n",
            "Collecting git+https://github.com/pybox2d/pybox2d\n",
            "  Cloning https://github.com/pybox2d/pybox2d to /tmp/pip-req-build-tqg5pdpf\n",
            "  Resolved https://github.com/pybox2d/pybox2d to commit 09643321fd363f0850087d1bde8af3f4afd82163\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting git+https://github.com/mimoralea/gym-bandits\n",
            "  Cloning https://github.com/mimoralea/gym-bandits to /tmp/pip-req-build-4l8kdrat\n",
            "  Resolved https://github.com/mimoralea/gym-bandits to commit f4b45d39e59eee3f6377ceb42466749af2cd77bd\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: gym>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from gym_bandits==0.1) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.2.3->gym_bandits==0.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.2.3->gym_bandits==0.1) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.2.3->gym_bandits==0.1) (0.0.8)\n",
            "Collecting git+https://github.com/mimoralea/gym-walk\n",
            "  Cloning https://github.com/mimoralea/gym-walk to /tmp/pip-req-build-qt6884s3\n",
            "  Resolved https://github.com/mimoralea/gym-walk to commit 5999016267d6de2f5a63307fb00dfd63de319ac1\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym_walk==0.0.2) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym_walk==0.0.2) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym_walk==0.0.2) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym_walk==0.0.2) (0.0.8)\n",
            "Collecting git+https://github.com/mimoralea/gym-aima\n",
            "  Cloning https://github.com/mimoralea/gym-aima to /tmp/pip-req-build-wjjm9b1r\n",
            "  Resolved https://github.com/mimoralea/gym-aima to commit 287c978be164ed6a3d36f3e3043bc0e41a25f165\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym_aima==0.0.1) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym_aima==0.0.1) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym_aima==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym_aima==0.0.1) (0.0.8)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.4.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/pybox2d/pybox2d /tmp/pip-req-build-tqg5pdpf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mimoralea/gym-bandits /tmp/pip-req-build-4l8kdrat\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mimoralea/gym-walk /tmp/pip-req-build-qt6884s3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mimoralea/gym-aima /tmp/pip-req-build-wjjm9b1r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV8fOPMvZNGT",
        "outputId": "fd2df462-1c9c-42c2-de26-8e7bf5ef45cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec 10 03:03:04 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0              27W /  70W |    165MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "I9FnwB1oZNGU"
      },
      "outputs": [],
      "source": [
        "import warnings ; warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "from collections import namedtuple, deque\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "from itertools import cycle, count\n",
        "from textwrap import wrap\n",
        "\n",
        "import matplotlib\n",
        "import subprocess\n",
        "import os.path\n",
        "import tempfile\n",
        "import random\n",
        "import base64\n",
        "import pprint\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import sys\n",
        "import gym\n",
        "import io\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from gym import wrappers\n",
        "from gym.wrappers import RecordVideo, RecordEpisodeStatistics\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def video_to_gif(video_path, gif_path):\n",
        "    clip = VideoFileClip(video_path)\n",
        "    clip.write_gif(gif_path, fps=7)\n",
        "    clip.close()\n",
        "\n",
        "from subprocess import check_output\n",
        "from IPython.display import HTML\n",
        "\n",
        "LEAVE_PRINT_EVERY_N_SECS = 120\n",
        "ERASE_LINE = '\\x1b[2K'\n",
        "EPS = 1e-6\n",
        "BEEP = lambda: os.system(\"printf '\\a'\")\n",
        "RESULTS_DIR = os.path.join('..', 'results')\n",
        "SEEDS = (12, 34, 56, 78, 90)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ws-Awu2WZNGU"
      },
      "outputs": [],
      "source": [
        "plt.style.use('fivethirtyeight')\n",
        "params = {\n",
        "    'figure.figsize': (15, 8),\n",
        "    'font.size': 24,\n",
        "    'legend.fontsize': 20,\n",
        "    'axes.titlesize': 28,\n",
        "    'axes.labelsize': 24,\n",
        "    'xtick.labelsize': 20,\n",
        "    'ytick.labelsize': 20\n",
        "}\n",
        "pylab.rcParams.update(params)\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqGl7CY2ZNGU",
        "outputId": "f18f03bd-acde-4fe9-faa6-87478a3f3acd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MkAeW8lOZNGV"
      },
      "outputs": [],
      "source": [
        "def get_make_env_fn(**kargs):\n",
        "    def make_env_fn(env_name, seed=None, render=None, record=False,\n",
        "                    unwrapped=False, monitor_mode=None,\n",
        "                    inner_wrappers=None, outer_wrappers=None):\n",
        "        mdir = tempfile.mkdtemp()\n",
        "        env = None\n",
        "        if render:\n",
        "            try:\n",
        "                env = gym.make(env_name, render=render)\n",
        "            except:\n",
        "                pass\n",
        "        if env is None:\n",
        "            env = gym.make(env_name)\n",
        "        if seed is not None: env.seed(seed)\n",
        "        env = env.unwrapped if unwrapped else env\n",
        "        if inner_wrappers:\n",
        "            for wrapper in inner_wrappers:\n",
        "                env = wrapper(env)\n",
        "        # Replace Monitor with new wrappers\n",
        "        if monitor_mode:\n",
        "            if record:\n",
        "                env = RecordVideo(env, mdir, episode_trigger=lambda x: record)\n",
        "            env = RecordEpisodeStatistics(env)\n",
        "        if outer_wrappers:\n",
        "            for wrapper in outer_wrappers:\n",
        "                env = wrapper(env)\n",
        "        return env\n",
        "    return make_env_fn, kargs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7ndUJPMyZNGV"
      },
      "outputs": [],
      "source": [
        "def get_videos_html_legacy(env_videos, title, max_n_videos=5):\n",
        "    videos = np.array(env_videos)\n",
        "    if len(videos) == 0:\n",
        "        return\n",
        "\n",
        "    n_videos = max(1, min(max_n_videos, len(videos)))\n",
        "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos > 1 else [-1,]\n",
        "    videos = videos[idxs,...]\n",
        "\n",
        "    strm = '<h2>{}<h2>'.format(title)\n",
        "    for video_path, meta_path in videos:\n",
        "        video = io.open(video_path, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "\n",
        "        with open(meta_path) as data_file:\n",
        "            meta = json.load(data_file)\n",
        "\n",
        "        html_tag = \"\"\"\n",
        "        <h3>{0}<h3/>\n",
        "        <video width=\"960\" height=\"540\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" />\n",
        "        </video>\"\"\"\n",
        "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n",
        "    return strm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_videos_html(env_videos, title, max_n_videos=5):\n",
        "    # Convert env_videos to numpy array for consistent indexing\n",
        "    videos = np.array(env_videos)\n",
        "    if len(videos) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    n_videos = max(1, min(max_n_videos, len(videos)))\n",
        "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos > 1 else [-1,]\n",
        "    videos = videos[idxs,...]\n",
        "\n",
        "    strm = f'<h2>{title}</h2>'\n",
        "    for video_path, meta_path in videos:\n",
        "        with io.open(video_path, 'rb') as video_file:\n",
        "            video = video_file.read()\n",
        "        encoded = base64.b64encode(video).decode('ascii')\n",
        "\n",
        "        with open(meta_path, 'r') as data_file:\n",
        "            meta = json.load(data_file)\n",
        "\n",
        "        html_tag = \"\"\"\n",
        "        <h3>{0}</h3>\n",
        "        <video width=\"960\" height=\"540\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" />\n",
        "        </video>\"\"\"\n",
        "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded)\n",
        "    return strm"
      ],
      "metadata": {
        "id": "vvQsjXedAWgX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iofouFdbZNGV"
      },
      "outputs": [],
      "source": [
        "def get_gif_html_legacy(env_videos, title, subtitle_eps=None, max_n_videos=4):\n",
        "    videos = np.array(env_videos)\n",
        "    if len(videos) == 0:\n",
        "        return\n",
        "\n",
        "    n_videos = max(1, min(max_n_videos, len(videos)))\n",
        "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos > 1 else [-1,]\n",
        "    videos = videos[idxs,...]\n",
        "\n",
        "    strm = '<h2>{}<h2>'.format(title)\n",
        "    for video_path, meta_path in videos:\n",
        "        basename = os.path.splitext(video_path)[0]\n",
        "        gif_path = basename + '.gif'\n",
        "        if not os.path.exists(gif_path):\n",
        "            ps = subprocess.Popen(\n",
        "                ('ffmpeg',\n",
        "                 '-i', video_path,\n",
        "                 '-r', '7',\n",
        "                 '-f', 'image2pipe',\n",
        "                 '-vcodec', 'ppm',\n",
        "                 '-crf', '20',\n",
        "                 '-vf', 'scale=512:-1',\n",
        "                 '-'),\n",
        "                stdout=subprocess.PIPE)\n",
        "            output = subprocess.check_output(\n",
        "                ('convert',\n",
        "                 '-coalesce',\n",
        "                 '-delay', '7',\n",
        "                 '-loop', '0',\n",
        "                 '-fuzz', '2%',\n",
        "                 '+dither',\n",
        "                 '-deconstruct',\n",
        "                 '-layers', 'Optimize',\n",
        "                 '-', gif_path),\n",
        "                stdin=ps.stdout)\n",
        "            ps.wait()\n",
        "\n",
        "        gif = io.open(gif_path, 'r+b').read()\n",
        "        encoded = base64.b64encode(gif)\n",
        "\n",
        "        with open(meta_path) as data_file:\n",
        "            meta = json.load(data_file)\n",
        "\n",
        "        html_tag = \"\"\"\n",
        "        <h3>{0}<h3/>\n",
        "        <img src=\"data:image/gif;base64,{1}\" />\"\"\"\n",
        "        prefix = 'Trial ' if subtitle_eps is None else 'Episode '\n",
        "        sufix = str(meta['episode_id'] if subtitle_eps is None \\\n",
        "                    else subtitle_eps[meta['episode_id']])\n",
        "        strm += html_tag.format(prefix + sufix, encoded.decode('ascii'))\n",
        "    return strm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def video_to_gif(video_path, gif_path, fps=7):\n",
        "    try:\n",
        "        clip = VideoFileClip(video_path)\n",
        "        clip.write_gif(gif_path, fps=fps)\n",
        "        clip.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting {video_path} to GIF: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def get_gif_html(env_videos, title, subtitle_eps=None, max_n_videos=4):\n",
        "    if len(env_videos) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    n_videos = max(1, min(max_n_videos, len(env_videos)))\n",
        "    idxs = np.linspace(0, len(env_videos) - 1, n_videos).astype(int) if n_videos > 1 else [-1,]\n",
        "    videos = [env_videos[i] for i in idxs]\n",
        "\n",
        "    strm = f'<h2>{title}<h2>'\n",
        "    for video_path, meta_path in videos:\n",
        "        basename = os.path.splitext(video_path)[0]\n",
        "        gif_path = basename + '.gif'\n",
        "        if not os.path.exists(gif_path):\n",
        "            # Use video_to_gif function to convert video to gif\n",
        "            if not video_to_gif(video_path, gif_path):\n",
        "                print(f\"Failed to create GIF for {video_path}\")\n",
        "                continue  # Skip this video if conversion failed\n",
        "\n",
        "        with open(gif_path, 'rb') as gif_file:\n",
        "            gif = gif_file.read()\n",
        "        encoded = base64.b64encode(gif).decode('ascii')\n",
        "\n",
        "        #with open(meta_path, 'r') as data_file:\n",
        "        #    meta = json.load(data_file)\n",
        "\n",
        "        html_tag = \"\"\"\n",
        "        <h3>{0}</h3>\n",
        "        <img src=\"data:image/gif;base64,{1}\" />\"\"\"\n",
        "        prefix = 'Trial ' if subtitle_eps is None else 'Episode '\n",
        "        #sufix = str(meta['episode_id'] if subtitle_eps is None else subtitle_eps[meta['episode_id']])\n",
        "        strm += html_tag.format(prefix ,encoded)#+ sufix, encoded)\n",
        "    return strm"
      ],
      "metadata": {
        "id": "QZecFMcFANm1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4D0si-QZNGV"
      },
      "source": [
        "# Dueling DDQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "y34fjNNCZNGW"
      },
      "outputs": [],
      "source": [
        "class FCQ(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 output_dim,\n",
        "                 hidden_dims=(32,32),\n",
        "                 activation_fc=F.relu):\n",
        "        super(FCQ, self).__init__()\n",
        "        self.activation_fc = activation_fc\n",
        "\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(len(hidden_dims)-1):\n",
        "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
        "            self.hidden_layers.append(hidden_layer)\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
        "\n",
        "        device = \"cpu\"\n",
        "        if torch.cuda.is_available():\n",
        "            device = \"cuda:0\"\n",
        "        self.device = torch.device(device)\n",
        "        self.to(self.device)\n",
        "\n",
        "    def _format(self, state):\n",
        "        x = state\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.tensor(x,\n",
        "                             device=self.device,\n",
        "                             dtype=torch.float32)\n",
        "            x = x.unsqueeze(0)\n",
        "        return x\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self._format(state)\n",
        "        x = self.activation_fc(self.input_layer(x))\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = self.activation_fc(hidden_layer(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "    def numpy_float_to_device(self, variable):\n",
        "        variable = torch.from_numpy(variable).float().to(self.device)\n",
        "        return variable\n",
        "\n",
        "    def load(self, experiences):\n",
        "        states, actions, new_states, rewards, is_terminals = experiences\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(actions).long().to(self.device)\n",
        "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
        "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
        "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
        "        return states, actions, new_states, rewards, is_terminals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SeGKTmXQZNGW"
      },
      "outputs": [],
      "source": [
        "class GreedyStrategy():\n",
        "    def __init__(self):\n",
        "        self.exploratory_action_taken = False\n",
        "\n",
        "    def select_action(self, model, state):\n",
        "        with torch.no_grad():\n",
        "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
        "            return np.argmax(q_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "69IqD8F9ZNGX"
      },
      "outputs": [],
      "source": [
        "class EGreedyStrategy():\n",
        "    def __init__(self, epsilon=0.1):\n",
        "        self.epsilon = epsilon\n",
        "        self.exploratory_action_taken = None\n",
        "\n",
        "    def select_action(self, model, state):\n",
        "        self.exploratory_action_taken = False\n",
        "        with torch.no_grad():\n",
        "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
        "\n",
        "        if np.random.rand() > self.epsilon:\n",
        "            action = np.argmax(q_values)\n",
        "        else:\n",
        "            action = np.random.randint(len(q_values))\n",
        "\n",
        "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1kcFGguwZNGX"
      },
      "outputs": [],
      "source": [
        "class EGreedyLinearStrategy():\n",
        "    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, max_steps=20000):\n",
        "        self.t = 0\n",
        "        self.epsilon = init_epsilon\n",
        "        self.init_epsilon = init_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.max_steps = max_steps\n",
        "        self.exploratory_action_taken = None\n",
        "\n",
        "    def _epsilon_update(self):\n",
        "        epsilon = 1 - self.t / self.max_steps\n",
        "        epsilon = (self.init_epsilon - self.min_epsilon) * epsilon + self.min_epsilon\n",
        "        epsilon = np.clip(epsilon, self.min_epsilon, self.init_epsilon)\n",
        "        self.t += 1\n",
        "        return epsilon\n",
        "\n",
        "    def select_action(self, model, state):\n",
        "        self.exploratory_action_taken = False\n",
        "        with torch.no_grad():\n",
        "            q_values = model(state).cpu().detach().data.numpy().ipynb_checkpoints/squeeze()\n",
        "\n",
        "        if np.random.rand() > self.epsilon:\n",
        "            action = np.argmax(q_values)\n",
        "        else:\n",
        "            action = np.random.randint(len(q_values))\n",
        "\n",
        "        self.epsilon = self._epsilon_update()\n",
        "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "eS_rA4XyZNGX"
      },
      "outputs": [],
      "source": [
        "class EGreedyExpStrategy():\n",
        "    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):\n",
        "        self.epsilon = init_epsilon\n",
        "        self.init_epsilon = init_epsilon\n",
        "        self.decay_steps = decay_steps\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.epsilons = 0.01 / np.logspace(-2, 0, decay_steps, endpoint=False) - 0.01\n",
        "        self.epsilons = self.epsilons * (init_epsilon - min_epsilon) + min_epsilon\n",
        "        self.t = 0\n",
        "        self.exploratory_action_taken = None\n",
        "\n",
        "    def _epsilon_update(self):\n",
        "        self.epsilon = self.min_epsilon if self.t >= self.decay_steps else self.epsilons[self.t]\n",
        "        self.t += 1\n",
        "        return self.epsilon\n",
        "\n",
        "    def select_action(self, model, state):\n",
        "        self.exploratory_action_taken = False\n",
        "        with torch.no_grad():\n",
        "            q_values = model(state).detach().cpu().data.numpy().squeeze()\n",
        "\n",
        "        if np.random.rand() > self.epsilon:\n",
        "            action = np.argmax(q_values)\n",
        "        else:\n",
        "            action = np.random.randint(len(q_values))\n",
        "\n",
        "        self._epsilon_update()\n",
        "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TIi-TTYdZNGX"
      },
      "outputs": [],
      "source": [
        "class SoftMaxStrategy():\n",
        "    def __init__(self,\n",
        "                 init_temp=1.0,\n",
        "                 min_temp=0.3,\n",
        "                 exploration_ratio=0.8,\n",
        "                 max_steps=25000):\n",
        "        self.t = 0\n",
        "        self.init_temp = init_temp\n",
        "        self.exploration_ratio = exploration_ratio\n",
        "        self.min_temp = min_temp\n",
        "        self.max_steps = max_steps\n",
        "        self.exploratory_action_taken = None\n",
        "\n",
        "    def _update_temp(self):\n",
        "        temp = 1 - self.t / (self.max_steps * self.exploration_ratio)\n",
        "        temp = (self.init_temp - self.min_temp) * temp + self.min_temp\n",
        "        temp = np.clip(temp, self.min_temp, self.init_temp)\n",
        "        self.t += 1\n",
        "        return temp\n",
        "\n",
        "    def select_action(self, model, state):\n",
        "        self.exploratory_action_taken = False\n",
        "        temp = self._update_temp()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
        "            scaled_qs = q_values/temp\n",
        "            norm_qs = scaled_qs - scaled_qs.max()\n",
        "            e = np.exp(norm_qs)\n",
        "            probs = e / np.sum(e)\n",
        "            assert np.isclose(probs.sum(), 1.0)\n",
        "\n",
        "        action = np.random.choice(np.arange(len(probs)), size=1, p=probs)[0]\n",
        "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ygykMLUIZNGX"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self,\n",
        "                 max_size=10000,\n",
        "                 batch_size=64):\n",
        "        self.ss_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
        "        self.as_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
        "        self.rs_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
        "        self.ps_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
        "        self.ds_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
        "\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "        self._idx = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def store(self, sample):\n",
        "        s, a, r, p, d = sample\n",
        "        self.ss_mem[self._idx] = s\n",
        "        self.as_mem[self._idx] = a\n",
        "        self.rs_mem[self._idx] = r\n",
        "        self.ps_mem[self._idx] = p\n",
        "        self.ds_mem[self._idx] = d\n",
        "\n",
        "        self._idx += 1\n",
        "        self._idx = self._idx % self.max_size\n",
        "\n",
        "        self.size += 1\n",
        "        self.size = min(self.size, self.max_size)\n",
        "\n",
        "    def sample(self, batch_size=None):\n",
        "        if batch_size == None:\n",
        "            batch_size = self.batch_size\n",
        "\n",
        "        idxs = np.random.choice(\n",
        "            self.size, batch_size, replace=False)\n",
        "        experiences = np.vstack(self.ss_mem[idxs]), \\\n",
        "                      np.vstack(self.as_mem[idxs]), \\\n",
        "                      np.vstack(self.rs_mem[idxs]), \\\n",
        "                      np.vstack(self.ps_mem[idxs]), \\\n",
        "                      np.vstack(self.ds_mem[idxs])\n",
        "        return experiences\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Rspdm76MZNGX"
      },
      "outputs": [],
      "source": [
        "class FCDuelingQ(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 output_dim,\n",
        "                 hidden_dims=(32,32),\n",
        "                 activation_fc=F.relu):\n",
        "        super(FCDuelingQ, self).__init__()\n",
        "        self.activation_fc = activation_fc\n",
        "\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(len(hidden_dims)-1):\n",
        "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
        "            self.hidden_layers.append(hidden_layer)\n",
        "        self.output_value = nn.Linear(hidden_dims[-1], 1)\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
        "\n",
        "        device = \"cpu\"\n",
        "        if torch.cuda.is_available():\n",
        "            device = \"cuda:0\"\n",
        "        self.device = torch.device(device)\n",
        "        self.to(self.device)\n",
        "\n",
        "    def _format(self, state):\n",
        "        x = state\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.tensor(x,\n",
        "                             device=self.device,\n",
        "                             dtype=torch.float32)\n",
        "            x = x.unsqueeze(0)\n",
        "        return x\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self._format(state)\n",
        "        x = self.activation_fc(self.input_layer(x))\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = self.activation_fc(hidden_layer(x))\n",
        "        a = self.output_layer(x)\n",
        "        v = self.output_value(x).expand_as(a)\n",
        "        q = v + a - a.mean(1, keepdim=True).expand_as(a)\n",
        "        return q\n",
        "\n",
        "    def numpy_float_to_device(self, variable):\n",
        "        variable = torch.from_numpy(variable).float().to(self.device)\n",
        "        return variable\n",
        "\n",
        "    def load(self, experiences):\n",
        "        states, actions, new_states, rewards, is_terminals = experiences\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(actions).long().to(self.device)\n",
        "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
        "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
        "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
        "        return states, actions, new_states, rewards, is_terminals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "r6-8jDcgZNGY"
      },
      "outputs": [],
      "source": [
        "class DuelingDDQN():\n",
        "    def __init__(self,\n",
        "                 replay_buffer_fn,\n",
        "                 value_model_fn,\n",
        "                 value_optimizer_fn,\n",
        "                 value_optimizer_lr,\n",
        "                 max_gradient_norm,\n",
        "                 training_strategy_fn,\n",
        "                 evaluation_strategy_fn,\n",
        "                 n_warmup_batches,\n",
        "                 update_target_every_steps,\n",
        "                 tau):\n",
        "        self.replay_buffer_fn = replay_buffer_fn\n",
        "        self.value_model_fn = value_model_fn\n",
        "        self.value_optimizer_fn = value_optimizer_fn\n",
        "        self.value_optimizer_lr = value_optimizer_lr\n",
        "        self.max_gradient_norm = max_gradient_norm\n",
        "        self.training_strategy_fn = training_strategy_fn\n",
        "        self.evaluation_strategy_fn = evaluation_strategy_fn\n",
        "        self.n_warmup_batches = n_warmup_batches\n",
        "        self.update_target_every_steps = update_target_every_steps\n",
        "        self.tau = tau\n",
        "\n",
        "    def optimize_model(self, experiences):\n",
        "        states, actions, rewards, next_states, is_terminals = experiences\n",
        "        batch_size = len(is_terminals)\n",
        "\n",
        "        argmax_a_q_sp = self.online_model(next_states).max(1)[1]\n",
        "        q_sp = self.target_model(next_states).detach()\n",
        "        max_a_q_sp = q_sp[\n",
        "            np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n",
        "        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n",
        "        q_sa = self.online_model(states).gather(1, actions)\n",
        "\n",
        "        td_error = q_sa - target_q_sa\n",
        "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
        "        self.value_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.online_model.parameters(),\n",
        "                                       self.max_gradient_norm)\n",
        "        self.value_optimizer.step()\n",
        "\n",
        "    def interaction_step(self, state, env):\n",
        "        action = self.training_strategy.select_action(self.online_model, state)\n",
        "        new_state, reward, is_terminal, info = env.step(action)\n",
        "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
        "        is_failure = is_terminal and not is_truncated\n",
        "        experience = (state, action, reward, new_state, float(is_failure))\n",
        "        self.replay_buffer.store(experience)\n",
        "        self.episode_reward[-1] += reward\n",
        "        self.episode_timestep[-1] += 1\n",
        "        self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n",
        "        return new_state, is_terminal\n",
        "\n",
        "    def update_network(self, tau=None):\n",
        "        tau = self.tau if tau is None else tau\n",
        "        for target, online in zip(self.target_model.parameters(),\n",
        "                                  self.online_model.parameters()):\n",
        "            target_ratio = (1.0 - tau) * target.data\n",
        "            online_ratio = tau * online.data\n",
        "            mixed_weights = target_ratio + online_ratio\n",
        "            target.data.copy_(mixed_weights)\n",
        "\n",
        "    def train(self, make_env_fn, make_env_kargs, seed, gamma,\n",
        "              max_minutes, max_episodes, goal_mean_100_reward):\n",
        "        training_start, last_debug_time = time.time(), float('-inf')\n",
        "\n",
        "        self.checkpoint_dir = tempfile.mkdtemp()\n",
        "        self.make_env_fn = make_env_fn\n",
        "        self.make_env_kargs = make_env_kargs\n",
        "        self.seed = seed\n",
        "        self.gamma = gamma\n",
        "\n",
        "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
        "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
        "\n",
        "        nS, nA = env.observation_space.shape[0], env.action_space.n\n",
        "        self.episode_timestep = []\n",
        "        self.episode_reward = []\n",
        "        self.episode_seconds = []\n",
        "        self.evaluation_scores = []\n",
        "        self.episode_exploration = []\n",
        "\n",
        "        self.target_model = self.value_model_fn(nS, nA)\n",
        "        self.online_model = self.value_model_fn(nS, nA)\n",
        "        self.update_network(tau=1.0)\n",
        "\n",
        "        self.value_optimizer = self.value_optimizer_fn(self.online_model,\n",
        "                                                       self.value_optimizer_lr)\n",
        "\n",
        "        self.replay_buffer = self.replay_buffer_fn()\n",
        "        self.training_strategy = training_strategy_fn()\n",
        "        self.evaluation_strategy = evaluation_strategy_fn()\n",
        "\n",
        "        result = np.empty((max_episodes, 5))\n",
        "        result[:] = np.nan\n",
        "        training_time = 0\n",
        "        for episode in range(1, max_episodes + 1):\n",
        "            episode_start = time.time()\n",
        "\n",
        "            state, is_terminal = env.reset(), False\n",
        "            self.episode_reward.append(0.0)\n",
        "            self.episode_timestep.append(0.0)\n",
        "            self.episode_exploration.append(0.0)\n",
        "\n",
        "            for step in count():\n",
        "                state, is_terminal = self.interaction_step(state, env)\n",
        "\n",
        "                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
        "                if len(self.replay_buffer) > min_samples:\n",
        "                    experiences = self.replay_buffer.sample()\n",
        "                    experiences = self.online_model.load(experiences)\n",
        "                    self.optimize_model(experiences)\n",
        "\n",
        "                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n",
        "                    self.update_network()\n",
        "\n",
        "                if is_terminal:\n",
        "                    gc.collect()\n",
        "                    break\n",
        "\n",
        "            # stats\n",
        "            episode_elapsed = time.time() - episode_start\n",
        "            self.episode_seconds.append(episode_elapsed)\n",
        "            training_time += episode_elapsed\n",
        "            evaluation_score, _ = self.evaluate(self.online_model, env)\n",
        "            self.save_checkpoint(episode-1, self.online_model)\n",
        "\n",
        "            total_step = int(np.sum(self.episode_timestep))\n",
        "            self.evaluation_scores.append(evaluation_score)\n",
        "\n",
        "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
        "            std_10_reward = np.std(self.episode_reward[-10:])\n",
        "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
        "            std_100_reward = np.std(self.episode_reward[-100:])\n",
        "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
        "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
        "            lst_100_exp_rat = np.array(\n",
        "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
        "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
        "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
        "\n",
        "            wallclock_elapsed = time.time() - training_start\n",
        "            result[episode-1] = total_step, mean_100_reward, \\\n",
        "                mean_100_eval_score, training_time, wallclock_elapsed\n",
        "\n",
        "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
        "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
        "            reached_max_episodes = episode >= max_episodes\n",
        "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
        "            training_is_over = reached_max_minutes or \\\n",
        "                               reached_max_episodes or \\\n",
        "                               reached_goal_mean_reward\n",
        "\n",
        "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
        "            debug_message = 'el {}, ep {:04}, ts {:06}, '\n",
        "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
        "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
        "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
        "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
        "            debug_message = debug_message.format(\n",
        "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward,\n",
        "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
        "                mean_100_eval_score, std_100_eval_score)\n",
        "            print(debug_message, end='\\r', flush=True)\n",
        "            if reached_debug_time or training_is_over:\n",
        "                print(ERASE_LINE + debug_message, flush=True)\n",
        "                last_debug_time = time.time()\n",
        "            if training_is_over:\n",
        "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
        "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
        "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
        "                break\n",
        "\n",
        "        final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n",
        "        wallclock_time = time.time() - training_start\n",
        "        print('Training complete.')\n",
        "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
        "              ' {:.2f}s wall-clock time.\\n'.format(\n",
        "                  final_eval_score, score_std, training_time, wallclock_time))\n",
        "        env.close() ; del env\n",
        "        self.get_cleaned_checkpoints()\n",
        "        return result, final_eval_score, training_time, wallclock_time\n",
        "\n",
        "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
        "        rs = []\n",
        "        for _ in range(n_episodes):\n",
        "            s, d = eval_env.reset(), False\n",
        "            rs.append(0)\n",
        "            for _ in count():\n",
        "                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n",
        "                s, r, d, _ = eval_env.step(a)\n",
        "                rs[-1] += r\n",
        "                if d: break\n",
        "        return np.mean(rs), np.std(rs)\n",
        "\n",
        "    def get_cleaned_checkpoints(self, n_checkpoints=5):\n",
        "        try:\n",
        "            return self.checkpoint_paths\n",
        "        except AttributeError:\n",
        "            self.checkpoint_paths = {}\n",
        "\n",
        "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
        "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
        "        last_ep = max(paths_dic.keys())\n",
        "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
        "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=int)-1\n",
        "\n",
        "        for idx, path in paths_dic.items():\n",
        "            if idx in checkpoint_idxs:\n",
        "                self.checkpoint_paths[idx] = path\n",
        "            else:\n",
        "                os.unlink(path)\n",
        "\n",
        "        return self.checkpoint_paths\n",
        "\n",
        "    def demo_last_legacy(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n",
        "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
        "\n",
        "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
        "        last_ep = max(checkpoint_paths.keys())\n",
        "        self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
        "\n",
        "        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n",
        "        env.close()\n",
        "        data = get_gif_html(env_videos=env.videos,\n",
        "                            title=title.format(self.__class__.__name__),\n",
        "                            max_n_videos=max_n_videos)\n",
        "        del env\n",
        "        return HTML(data=data)\n",
        "\n",
        "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n",
        "        # Create a temporary directory for video storage\n",
        "        temp_dir = os.path.join(os.getcwd(), 'temp_videos')\n",
        "        if not os.path.exists(temp_dir):\n",
        "            os.makedirs(temp_dir)\n",
        "\n",
        "        # Make environment with video recording\n",
        "        env = self.make_env_fn(**self.make_env_kargs,\n",
        "                              monitor_mode='evaluation',\n",
        "                              render=True)\n",
        "        env = RecordVideo(env, temp_dir, episode_trigger=lambda x: True)  # Record all episodes\n",
        "        env = RecordEpisodeStatistics(env)  # To capture episode stats\n",
        "\n",
        "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
        "        last_ep = max(checkpoint_paths.keys())\n",
        "        self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
        "\n",
        "        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n",
        "        env.close()\n",
        "\n",
        "        # Gather video and metadata files\n",
        "        videos = []\n",
        "        for video_file in os.listdir(temp_dir):\n",
        "            if video_file.endswith('.mp4'):\n",
        "                video_path = os.path.join(temp_dir, video_file)\n",
        "                meta_path = os.path.join(temp_dir, os.path.splitext(video_file)[0] + '.json')\n",
        "                videos.append((video_path, meta_path))\n",
        "\n",
        "        # Generate GIF HTML\n",
        "        data = get_gif_html(env_videos=videos,\n",
        "                            title=title.format(self.__class__.__name__),\n",
        "                            max_n_videos=max_n_videos)\n",
        "\n",
        "        # Clean up temporary directory\n",
        "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "\n",
        "        del env\n",
        "        return HTML(data=data)\n",
        "\n",
        "    def demo_progression_legacy(self, title='{} Agent progression', max_n_videos=5):\n",
        "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
        "\n",
        "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
        "        for i in sorted(checkpoint_paths.keys()):\n",
        "            self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
        "            self.evaluate(self.online_model, env, n_episodes=1)\n",
        "\n",
        "        env.close()\n",
        "        data = get_gif_html(env_videos=env.videos,\n",
        "                            title=title.format(self.__class__.__name__),\n",
        "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
        "                            max_n_videos=max_n_videos)\n",
        "        del env\n",
        "        return HTML(data=data)\n",
        "\n",
        "    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n",
        "        # Create a temporary directory for video storage\n",
        "        temp_dir = os.path.join(os.getcwd(), 'temp_videos_progression')\n",
        "        if not os.path.exists(temp_dir):\n",
        "            os.makedirs(temp_dir)\n",
        "\n",
        "        # Make environment with video recording\n",
        "        env = self.make_env_fn(**self.make_env_kargs,\n",
        "                              monitor_mode='evaluation',\n",
        "                              render=True)\n",
        "        env = RecordVideo(env, temp_dir, episode_trigger=lambda x: True)  # Record all episodes\n",
        "        env = RecordEpisodeStatistics(env)  # To capture episode stats\n",
        "\n",
        "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
        "        for i in sorted(checkpoint_paths.keys()):\n",
        "            self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
        "            self.evaluate(self.online_model, env, n_episodes=1)\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        # Gather video and metadata files\n",
        "        videos = []\n",
        "        for video_file in sorted(os.listdir(temp_dir)):\n",
        "            if video_file.endswith('.mp4'):\n",
        "                video_path = os.path.join(temp_dir, video_file)\n",
        "                meta_path = os.path.join(temp_dir, os.path.splitext(video_file)[0] + '.json')\n",
        "                videos.append((video_path, meta_path))\n",
        "\n",
        "        # Generate GIF HTML\n",
        "        data = get_gif_html(env_videos=videos,\n",
        "                            title=title.format(self.__class__.__name__),\n",
        "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
        "                            max_n_videos=max_n_videos)\n",
        "\n",
        "        # Clean up temporary directory\n",
        "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "\n",
        "        del env\n",
        "        return HTML(data=data)\n",
        "\n",
        "    def save_checkpoint(self, episode_idx, model):\n",
        "        torch.save(model.state_dict(),\n",
        "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLxfZgD-ZNGY",
        "outputId": "68478a39-86dd-4dbc-87f3-4d2b9b58ed64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2Kel 00:00:00, ep 0000, ts 000017, ar 10 017.0±000.0, 100 017.0±000.0, ex 100 0.4±0.0, ev 009.0±000.0\n",
            "\u001b[2Kel 00:02:02, ep 0146, ts 014658, ar 10 197.0±096.4, 100 133.7±104.7, ex 100 0.3±0.1, ev 280.5±103.0\n",
            "\u001b[2Kel 00:04:05, ep 0199, ts 033978, ar 10 397.9±151.6, 100 291.6±141.8, ex 100 0.2±0.1, ev 378.7±118.0\n",
            "\u001b[2Kel 00:06:08, ep 0240, ts 052776, ar 10 460.5±082.4, 100 393.0±140.2, ex 100 0.2±0.0, ev 447.1±100.3\n",
            "\u001b[2Kel 00:06:53, ep 0255, ts 059613, ar 10 437.9±102.8, 100 429.3±119.7, ex 100 0.2±0.0, ev 476.7±064.4\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 498.43±15.62 in 373.91s training time, 435.42s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000029, ar 10 029.0±000.0, 100 029.0±000.0, ex 100 0.6±0.0, ev 010.0±000.0\n",
            "\u001b[2Kel 00:02:01, ep 0167, ts 015344, ar 10 209.9±055.6, 100 132.6±082.7, ex 100 0.3±0.1, ev 284.2±096.1\n",
            "\u001b[2Kel 00:04:02, ep 0221, ts 034735, ar 10 500.0±000.0, 100 283.1±145.6, ex 100 0.2±0.0, ev 367.6±118.1\n",
            "\u001b[2Kel 00:06:04, ep 0262, ts 053652, ar 10 498.0±006.0, 100 393.2±151.7, ex 100 0.2±0.0, ev 445.9±099.0\n",
            "\u001b[2Kel 00:06:49, ep 0279, ts 060345, ar 10 380.4±159.3, 100 426.7±138.3, ex 100 0.2±0.0, ev 475.0±076.3\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 497.50±24.87 in 366.29s training time, 430.52s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000019, ar 10 019.0±000.0, 100 019.0±000.0, ex 100 0.5±0.0, ev 010.0±000.0\n",
            "\u001b[2Kel 00:02:00, ep 0168, ts 015608, ar 10 200.6±040.4, 100 136.5±091.4, ex 100 0.3±0.1, ev 263.0±097.4\n",
            "\u001b[2Kel 00:04:01, ep 0224, ts 034821, ar 10 500.0±000.0, 100 284.1±140.8, ex 100 0.2±0.1, ev 371.1±126.1\n",
            "\u001b[2Kel 00:06:01, ep 0265, ts 053452, ar 10 428.6±135.0, 100 384.3±146.3, ex 100 0.2±0.0, ev 442.9±112.1\n",
            "\u001b[2Kel 00:06:40, ep 0280, ts 059215, ar 10 352.4±159.1, 100 410.0±137.3, ex 100 0.2±0.0, ev 477.3±071.9\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 469.82±71.93 in 359.42s training time, 421.11s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.2±0.0, ev 010.0±000.0\n",
            "\u001b[2Kel 00:02:01, ep 0164, ts 016496, ar 10 246.1±099.3, 100 146.0±097.4, ex 100 0.3±0.1, ev 274.6±099.9\n",
            "\u001b[2Kel 00:04:03, ep 0215, ts 035291, ar 10 425.0±157.2, 100 288.4±154.2, ex 100 0.2±0.1, ev 392.1±122.3\n",
            "\u001b[2Kel 00:06:03, ep 0254, ts 053557, ar 10 476.1±071.7, 100 395.2±145.1, ex 100 0.2±0.0, ev 466.1±085.7\n",
            "\u001b[2Kel 00:06:22, ep 0260, ts 056557, ar 10 476.1±071.7, 100 409.0±140.9, ex 100 0.2±0.0, ev 476.6±072.7\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 343.63s training time, 404.58s wall-clock time.\n",
            "\n",
            "\u001b[2Kel 00:00:00, ep 0000, ts 000028, ar 10 028.0±000.0, 100 028.0±000.0, ex 100 0.4±0.0, ev 034.0±000.0\n",
            "\u001b[2Kel 00:02:01, ep 0165, ts 016327, ar 10 251.6±102.5, 100 145.3±105.2, ex 100 0.3±0.1, ev 316.3±102.5\n",
            "\u001b[2Kel 00:04:04, ep 0224, ts 035623, ar 10 500.0±000.0, 100 287.0±138.9, ex 100 0.2±0.0, ev 375.5±112.5\n",
            "\u001b[2Kel 00:06:04, ep 0263, ts 054238, ar 10 500.0±000.0, 100 383.6±144.9, ex 100 0.2±0.0, ev 441.2±095.0\n",
            "\u001b[2Kel 00:07:09, ep 0283, ts 063998, ar 10 500.0±000.0, 100 438.9±113.6, ex 100 0.2±0.0, ev 476.8±066.8\n",
            "--> reached_goal_mean_reward ✓\n",
            "Training complete.\n",
            "Final evaluation score 500.00±0.00 in 388.18s training time, 451.86s wall-clock time.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dueling_ddqn_results = []\n",
        "dueling_ddqn_agents, best_dueling_ddqn_agent_key, best_eval_score = {}, None, float('-inf')\n",
        "for seed in SEEDS:\n",
        "    environment_settings = {\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'gamma': 1.00,\n",
        "        'max_minutes': 20,\n",
        "        'max_episodes': 10000,\n",
        "        'goal_mean_100_reward': 475\n",
        "    }\n",
        "\n",
        "    # value_model_fn = lambda nS, nA: FCQ(nS, nA, hidden_dims=(512,128))\n",
        "    value_model_fn = lambda nS, nA: FCDuelingQ(nS, nA, hidden_dims=(512,128))\n",
        "    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
        "    value_optimizer_lr = 0.0005\n",
        "    max_gradient_norm = float('inf')\n",
        "\n",
        "    training_strategy_fn = lambda: EGreedyExpStrategy(init_epsilon=1.0,\n",
        "                                                      min_epsilon=0.3,\n",
        "                                                      decay_steps=20000)\n",
        "    evaluation_strategy_fn = lambda: GreedyStrategy()\n",
        "\n",
        "    replay_buffer_fn = lambda: ReplayBuffer(max_size=50000, batch_size=64)\n",
        "    n_warmup_batches = 5\n",
        "    update_target_every_steps = 1\n",
        "    tau = 0.1\n",
        "\n",
        "    env_name, gamma, max_minutes, \\\n",
        "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
        "    agent = DuelingDDQN(replay_buffer_fn,\n",
        "                        value_model_fn,\n",
        "                        value_optimizer_fn,\n",
        "                        value_optimizer_lr,\n",
        "                        max_gradient_norm,\n",
        "                        training_strategy_fn,\n",
        "                        evaluation_strategy_fn,\n",
        "                        n_warmup_batches,\n",
        "                        update_target_every_steps,\n",
        "                        tau)\n",
        "\n",
        "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
        "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
        "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
        "    dueling_ddqn_results.append(result)\n",
        "    dueling_ddqn_agents[seed] = agent\n",
        "    if final_eval_score > best_eval_score:\n",
        "        best_eval_score = final_eval_score\n",
        "        best_dueling_ddqn_agent_key = seed\n",
        "dueling_ddqn_results = np.array(dueling_ddqn_results)\n",
        "_ = BEEP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "e0igxka-ZNGY",
        "outputId": "747b98e6-7057-4c28-f20d-da645be34aa3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/temp_videos_progression/rl-video-episode-0.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-ae2f77543189>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdueling_ddqn_agents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_dueling_ddqn_agent_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdemo_progression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-dc8eca3cd080>\u001b[0m in \u001b[0;36mdemo_progression\u001b[0;34m(self, title, max_n_videos)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Generate GIF HTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         data = get_gif_html(env_videos=videos,\n\u001b[0m\u001b[1;32m    317\u001b[0m                             \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                             \u001b[0msubtitle_eps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-4c79b5baa9dd>\u001b[0m in \u001b[0;36mget_gif_html\u001b[0;34m(env_videos, title, subtitle_eps, max_n_videos)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgif\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/temp_videos_progression/rl-video-episode-0.json'"
          ]
        }
      ],
      "source": [
        "dueling_ddqn_agents[best_dueling_ddqn_agent_key].demo_progression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYmk9sarZNGZ"
      },
      "outputs": [],
      "source": [
        "dueling_ddqn_agents[best_dueling_ddqn_agent_key].demo_last()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lspjQXd7ZNGZ"
      },
      "outputs": [],
      "source": [
        "ddqn_root_dir = os.path.join(RESULTS_DIR, 'ddqn')\n",
        "ddqn_x = np.load(os.path.join(ddqn_root_dir, 'x.npy'))\n",
        "\n",
        "ddqn_max_r = np.load(os.path.join(ddqn_root_dir, 'max_r.npy'))\n",
        "ddqn_min_r = np.load(os.path.join(ddqn_root_dir, 'min_r.npy'))\n",
        "ddqn_mean_r = np.load(os.path.join(ddqn_root_dir, 'mean_r.npy'))\n",
        "\n",
        "ddqn_max_s = np.load(os.path.join(ddqn_root_dir, 'max_s.npy'))\n",
        "ddqn_min_s = np.load(os.path.join(ddqn_root_dir, 'min_s.npy'))\n",
        "ddqn_mean_s = np.load(os.path.join(ddqn_root_dir, 'mean_s.npy'))\n",
        "\n",
        "ddqn_max_t = np.load(os.path.join(ddqn_root_dir, 'max_t.npy'))\n",
        "ddqn_min_t = np.load(os.path.join(ddqn_root_dir, 'min_t.npy'))\n",
        "ddqn_mean_t = np.load(os.path.join(ddqn_root_dir, 'mean_t.npy'))\n",
        "\n",
        "ddqn_max_sec = np.load(os.path.join(ddqn_root_dir, 'max_sec.npy'))\n",
        "ddqn_min_sec = np.load(os.path.join(ddqn_root_dir, 'min_sec.npy'))\n",
        "ddqn_mean_sec = np.load(os.path.join(ddqn_root_dir, 'mean_sec.npy'))\n",
        "\n",
        "ddqn_max_rt = np.load(os.path.join(ddqn_root_dir, 'max_rt.npy'))\n",
        "ddqn_min_rt = np.load(os.path.join(ddqn_root_dir, 'min_rt.npy'))\n",
        "ddqn_mean_rt = np.load(os.path.join(ddqn_root_dir, 'mean_rt.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHzRzZHtZNGZ"
      },
      "outputs": [],
      "source": [
        "dueling_ddqn_max_t, dueling_ddqn_max_r, dueling_ddqn_max_s, \\\n",
        "dueling_ddqn_max_sec, dueling_ddqn_max_rt = np.max(dueling_ddqn_results, axis=0).T\n",
        "dueling_ddqn_min_t, dueling_ddqn_min_r, dueling_ddqn_min_s, \\\n",
        "dueling_ddqn_min_sec, dueling_ddqn_min_rt = np.min(dueling_ddqn_results, axis=0).T\n",
        "dueling_ddqn_mean_t, dueling_ddqn_mean_r, dueling_ddqn_mean_s, \\\n",
        "dueling_ddqn_mean_sec, dueling_ddqn_mean_rt = np.mean(dueling_ddqn_results, axis=0).T\n",
        "dueling_ddqn_x = np.arange(np.max(\n",
        "    (len(dueling_ddqn_mean_s), len(ddqn_mean_s))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26ukKZ3DZNGZ"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5, 1, figsize=(15,30), sharey=False, sharex=True)\n",
        "\n",
        "# DDQN\n",
        "axs[0].plot(ddqn_max_r, 'g', linewidth=1)\n",
        "axs[0].plot(ddqn_min_r, 'g', linewidth=1)\n",
        "axs[0].plot(ddqn_mean_r, 'g-.', label='DDQN', linewidth=2)\n",
        "axs[0].fill_between(ddqn_x, ddqn_min_r, ddqn_max_r, facecolor='g', alpha=0.3)\n",
        "\n",
        "axs[1].plot(ddqn_max_s, 'g', linewidth=1)\n",
        "axs[1].plot(ddqn_min_s, 'g', linewidth=1)\n",
        "axs[1].plot(ddqn_mean_s, 'g-.', label='DDQN', linewidth=2)\n",
        "axs[1].fill_between(ddqn_x, ddqn_min_s, ddqn_max_s, facecolor='g', alpha=0.3)\n",
        "\n",
        "axs[2].plot(ddqn_max_t, 'g', linewidth=1)\n",
        "axs[2].plot(ddqn_min_t, 'g', linewidth=1)\n",
        "axs[2].plot(ddqn_mean_t, 'g-.', label='DDQN', linewidth=2)\n",
        "axs[2].fill_between(ddqn_x, ddqn_min_t, ddqn_max_t, facecolor='g', alpha=0.3)\n",
        "\n",
        "axs[3].plot(ddqn_max_sec, 'g', linewidth=1)\n",
        "axs[3].plot(ddqn_min_sec, 'g', linewidth=1)\n",
        "axs[3].plot(ddqn_mean_sec, 'g-.', label='DDQN', linewidth=2)\n",
        "axs[3].fill_between(ddqn_x, ddqn_min_sec, ddqn_max_sec, facecolor='g', alpha=0.3)\n",
        "\n",
        "axs[4].plot(ddqn_max_rt, 'g', linewidth=1)\n",
        "axs[4].plot(ddqn_min_rt, 'g', linewidth=1)\n",
        "axs[4].plot(ddqn_mean_rt, 'g-.', label='DDQN', linewidth=2)\n",
        "axs[4].fill_between(ddqn_x, ddqn_min_rt, ddqn_max_rt, facecolor='g', alpha=0.3)\n",
        "\n",
        "# Dueling DDQN\n",
        "axs[0].plot(dueling_ddqn_max_r, 'r', linewidth=1)\n",
        "axs[0].plot(dueling_ddqn_min_r, 'r', linewidth=1)\n",
        "axs[0].plot(dueling_ddqn_mean_r, 'r:', label='Dueling DDQN', linewidth=2)\n",
        "axs[0].fill_between(\n",
        "    dueling_ddqn_x, dueling_ddqn_min_r, dueling_ddqn_max_r, facecolor='r', alpha=0.3)\n",
        "\n",
        "axs[1].plot(dueling_ddqn_max_s, 'r', linewidth=1)\n",
        "axs[1].plot(dueling_ddqn_min_s, 'r', linewidth=1)\n",
        "axs[1].plot(dueling_ddqn_mean_s, 'r:', label='Dueling DDQN', linewidth=2)\n",
        "axs[1].fill_between(\n",
        "    dueling_ddqn_x, dueling_ddqn_min_s, dueling_ddqn_max_s, facecolor='r', alpha=0.3)\n",
        "\n",
        "axs[2].plot(dueling_ddqn_max_t, 'r', linewidth=1)\n",
        "axs[2].plot(dueling_ddqn_min_t, 'r', linewidth=1)\n",
        "axs[2].plot(dueling_ddqn_mean_t, 'r:', label='Dueling DDQN', linewidth=2)\n",
        "axs[2].fill_between(\n",
        "    dueling_ddqn_x, dueling_ddqn_min_t, dueling_ddqn_max_t, facecolor='r', alpha=0.3)\n",
        "\n",
        "axs[3].plot(dueling_ddqn_max_sec, 'r', linewidth=1)\n",
        "axs[3].plot(dueling_ddqn_min_sec, 'r', linewidth=1)\n",
        "axs[3].plot(dueling_ddqn_mean_sec, 'r:', label='Dueling DDQN', linewidth=2)\n",
        "axs[3].fill_between(\n",
        "    dueling_ddqn_x, dueling_ddqn_min_sec, dueling_ddqn_max_sec, facecolor='r', alpha=0.3)\n",
        "\n",
        "axs[4].plot(dueling_ddqn_max_rt, 'r', linewidth=1)\n",
        "axs[4].plot(dueling_ddqn_min_rt, 'r', linewidth=1)\n",
        "axs[4].plot(dueling_ddqn_mean_rt, 'r:', label='Dueling DDQN', linewidth=2)\n",
        "axs[4].fill_between(\n",
        "    dueling_ddqn_x, dueling_ddqn_min_rt, dueling_ddqn_max_rt, facecolor='r', alpha=0.3)\n",
        "\n",
        "# ALL\n",
        "axs[0].set_title('Moving Avg Reward (Training)')\n",
        "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
        "axs[2].set_title('Total Steps')\n",
        "axs[3].set_title('Training Time')\n",
        "axs[4].set_title('Wall-clock Time')\n",
        "plt.xlabel('Episodes')\n",
        "axs[0].legend(loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__SGAOPIZNGZ"
      },
      "outputs": [],
      "source": [
        "dueling_ddqn_root_dir = os.path.join(RESULTS_DIR, 'dueling_ddqn')\n",
        "not os.path.exists(dueling_ddqn_root_dir) and os.makedirs(dueling_ddqn_root_dir)\n",
        "\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'x'), dueling_ddqn_x)\n",
        "\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'max_r'), dueling_ddqn_max_r)\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'min_r'), dueling_ddqn_min_r)\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'mean_r'), dueling_ddqn_mean_r)\n",
        "\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'max_s'), dueling_ddqn_max_s)\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'min_s'), dueling_ddqn_min_s )\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'mean_s'), dueling_ddqn_mean_s)\n",
        "\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'max_t'), dueling_ddqn_max_t)\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'min_t'), dueling_ddqn_min_t)\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'mean_t'), dueling_ddqn_mean_t)\n",
        "\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'max_sec'), dueling_ddqn_max_sec)\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'min_sec'), dueling_ddqn_min_sec)\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'mean_sec'), dueling_ddqn_mean_sec)\n",
        "\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'max_rt'), dueling_ddqn_max_rt)\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'min_rt'), dueling_ddqn_min_rt)\n",
        "np.save(os.path.join(dueling_ddqn_root_dir, 'mean_rt'), dueling_ddqn_mean_rt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmBeivDoZNGZ"
      },
      "outputs": [],
      "source": [
        "env = make_env_fn(**make_env_kargs, seed=123, monitor_mode='evaluation')\n",
        "state = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "env.close()\n",
        "del env\n",
        "print(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roXSdmVCZNGZ"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title(\"State s=\" + str(np.round(state,2)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TD_M5uqZNGZ"
      },
      "outputs": [],
      "source": [
        "q_values = dueling_ddqn_agents[best_dueling_ddqn_agent_key].online_model(state).detach().cpu().numpy()[0]\n",
        "print(q_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nch02MNXZNGZ"
      },
      "outputs": [],
      "source": [
        "q_s = q_values\n",
        "v_s = q_values.mean()\n",
        "a_s = q_values - q_values.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so8BnQMgZNGa"
      },
      "outputs": [],
      "source": [
        "plt.bar(('Left (idx=0)','Right (idx=1)'), q_s)\n",
        "plt.xlabel('Action')\n",
        "plt.ylabel('Estimate')\n",
        "plt.title(\"Action-value function, Q(\" + str(np.round(state,2)) + \")\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-W73OJtZNGa"
      },
      "outputs": [],
      "source": [
        "plt.bar('s='+str(np.round(state,2)), v_s, width=0.1)\n",
        "plt.xlabel('State')\n",
        "plt.ylabel('Estimate')\n",
        "plt.title(\"State-value function, V(\"+str(np.round(state,2))+\")\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brdIUqGaZNGa"
      },
      "outputs": [],
      "source": [
        "plt.bar(('Left (idx=0)','Right (idx=1)'), a_s)\n",
        "plt.xlabel('Action')\n",
        "plt.ylabel('Estimate')\n",
        "plt.title(\"Advantage function, (\" + str(np.round(state,2)) + \")\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgvcC2gLZNGa"
      },
      "outputs": [],
      "source": [
        "env = make_env_fn(**make_env_kargs, seed=123, monitor_mode='evaluation')\n",
        "\n",
        "state, states, imgs, t = env.reset(), [], [], False\n",
        "while not t:\n",
        "    states.append(state)\n",
        "    state, r, t, _ = env.step(0)\n",
        "    imgs.append(env.render(mode='rgb_array'))\n",
        "\n",
        "env.close()\n",
        "del env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5s2FsnyZNGa"
      },
      "outputs": [],
      "source": [
        "states[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA0SRk1VZNGa"
      },
      "outputs": [],
      "source": [
        "plt.imshow(imgs[-2])\n",
        "plt.axis('off')\n",
        "plt.title(\"State s=\" + str(np.round(state,2)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiq-NZCOZNGa"
      },
      "outputs": [],
      "source": [
        "q_values = dueling_ddqn_agents[best_dueling_ddqn_agent_key].online_model(state).detach().cpu().numpy()[0]\n",
        "print(q_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpzUXPWZZNGa"
      },
      "outputs": [],
      "source": [
        "q_s = q_values\n",
        "v_s = q_values.mean()\n",
        "a_s = q_values - q_values.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqXmrxKlZNGb"
      },
      "outputs": [],
      "source": [
        "plt.bar(('Left (idx=0)','Right (idx=1)'), q_s)\n",
        "plt.xlabel('Action')\n",
        "plt.ylabel('Estimate')\n",
        "plt.title(\"Action-value function, Q(\" + str(np.round(state,2)) + \")\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjxR5RUUZNGb"
      },
      "outputs": [],
      "source": [
        "plt.bar('s='+str(np.round(state,2)), v_s, width=0.1)\n",
        "plt.xlabel('State')\n",
        "plt.ylabel('Estimate')\n",
        "plt.title(\"State-value function, V(\"+str(np.round(state,2))+\")\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RclvpdiFZNGb"
      },
      "outputs": [],
      "source": [
        "plt.bar(('Left (idx=0)','Right (idx=1)'), a_s)\n",
        "plt.xlabel('Action')\n",
        "plt.ylabel('Estimate')\n",
        "plt.title(\"Advantage function, (\" + str(np.round(state,2)) + \")\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZp6JUQ_ZNGb"
      },
      "outputs": [],
      "source": [
        "env = make_env_fn(**make_env_kargs, seed=123, monitor_mode='evaluation')\n",
        "\n",
        "states = []\n",
        "for agent in dueling_ddqn_agents.values():\n",
        "    for episode in range(100):\n",
        "        state, done = env.reset(), False\n",
        "        while not done:\n",
        "            states.append(state)\n",
        "            action = agent.evaluation_strategy.select_action(agent.online_model, state)\n",
        "            state, _, done, _ = env.step(action)\n",
        "env.close()\n",
        "del env\n",
        "\n",
        "x = np.array(states)[:,0]\n",
        "xd = np.array(states)[:,1]\n",
        "a = np.array(states)[:,2]\n",
        "ad = np.array(states)[:,3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RshHGNGZNGb"
      },
      "outputs": [],
      "source": [
        "parts = plt.violinplot((x, xd, a, ad),\n",
        "                       vert=False, showmeans=False, showmedians=False, showextrema=False)\n",
        "\n",
        "colors = ['red','green','yellow','blue']\n",
        "for i, pc in enumerate(parts['bodies']):\n",
        "    pc.set_facecolor(colors[i])\n",
        "    pc.set_edgecolor(colors[i])\n",
        "    pc.set_alpha(0.5)\n",
        "\n",
        "plt.yticks(range(1,5), [\"cart position\", \"cart velocity\", \"pole angle\", \"pole velocity\"])\n",
        "plt.yticks(rotation=45)\n",
        "plt.title('Range of state-variable values for ' + str(\n",
        "    dueling_ddqn_agents[best_dueling_ddqn_agent_key].__class__.__name__))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JOIlroxZNGb"
      },
      "source": [
        "# PER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jTkmnSPZNGb"
      },
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer():\n",
        "    def __init__(self,\n",
        "                 max_samples=10000,\n",
        "                 batch_size=64,\n",
        "                 rank_based=False,\n",
        "                 alpha=0.6,\n",
        "                 beta0=0.1,\n",
        "                 beta_rate=0.99992):\n",
        "        self.max_samples = max_samples\n",
        "        self.memory = np.empty(shape=(self.max_samples, 2), dtype=np.ndarray)\n",
        "        self.batch_size = batch_size\n",
        "        self.n_entries = 0\n",
        "        self.next_index = 0\n",
        "        self.td_error_index = 0\n",
        "        self.sample_index = 1\n",
        "        self.rank_based = rank_based # if not rank_based, then proportional\n",
        "        self.alpha = alpha # how much prioritization to use 0 is uniform (no priority), 1 is full priority\n",
        "        self.beta = beta0 # bias correction 0 is no correction 1 is full correction\n",
        "        self.beta0 = beta0 # beta0 is just beta's initial value\n",
        "        self.beta_rate = beta_rate\n",
        "\n",
        "    def update(self, idxs, td_errors):\n",
        "        self.memory[idxs, self.td_error_index] = np.abs(td_errors)\n",
        "        if self.rank_based:\n",
        "            sorted_arg = self.memory[:self.n_entries, self.td_error_index].argsort()[::-1]\n",
        "            self.memory[:self.n_entries] = self.memory[sorted_arg]\n",
        "\n",
        "    def store(self, sample):\n",
        "        priority = 1.0\n",
        "        if self.n_entries > 0:\n",
        "            priority = self.memory[\n",
        "                :self.n_entries,\n",
        "                self.td_error_index].max()\n",
        "        self.memory[self.next_index,\n",
        "                    self.td_error_index] = priority\n",
        "        self.memory[self.next_index,\n",
        "                    self.sample_index] = np.array(sample)\n",
        "        self.n_entries = min(self.n_entries + 1, self.max_samples)\n",
        "        self.next_index += 1\n",
        "        self.next_index = self.next_index % self.max_samples\n",
        "\n",
        "    def _update_beta(self):\n",
        "        self.beta = min(1.0, self.beta * self.beta_rate**-1)\n",
        "        return self.beta\n",
        "\n",
        "    def sample(self, batch_size=None):\n",
        "        batch_size = self.batch_size if batch_size == None else batch_size\n",
        "        self._update_beta()\n",
        "        entries = self.memory[:self.n_entries]\n",
        "\n",
        "        if self.rank_based:\n",
        "            priorities = 1/(np.arange(self.n_entries) + 1)\n",
        "        else: # proportional\n",
        "            priorities = entries[:, self.td_error_index] + EPS\n",
        "        scaled_priorities = priorities**self.alpha\n",
        "        probs = np.array(scaled_priorities/np.sum(scaled_priorities), dtype=np.float64)\n",
        "\n",
        "        weights = (self.n_entries * probs)**-self.beta\n",
        "        normalized_weights = weights/weights.max()\n",
        "        idxs = np.random.choice(self.n_entries, batch_size, replace=False, p=probs)\n",
        "        samples = np.array([entries[idx] for idx in idxs])\n",
        "\n",
        "        samples_stacks = [np.vstack(batch_type) for batch_type in np.vstack(samples[:, self.sample_index]).T]\n",
        "        idxs_stack = np.vstack(idxs)\n",
        "        weights_stack = np.vstack(normalized_weights[idxs])\n",
        "        return idxs_stack, weights_stack, samples_stacks\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_entries\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.memory[:self.n_entries])\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.memory[:self.n_entries])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mby6ArCZNGc"
      },
      "outputs": [],
      "source": [
        "b = PrioritizedReplayBuffer()\n",
        "plt.plot([b._update_beta() for _ in range(100000)])\n",
        "plt.title('PER Beta')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34e2SL28ZNGc"
      },
      "outputs": [],
      "source": [
        "class PER():\n",
        "    def __init__(self,\n",
        "                 replay_buffer_fn,\n",
        "                 value_model_fn,\n",
        "                 value_optimizer_fn,\n",
        "                 value_optimizer_lr,\n",
        "                 max_gradient_norm,\n",
        "                 training_strategy_fn,\n",
        "                 evaluation_strategy_fn,\n",
        "                 n_warmup_batches,\n",
        "                 update_target_every_steps,\n",
        "                 tau):\n",
        "        self.replay_buffer_fn = replay_buffer_fn\n",
        "        self.value_model_fn = value_model_fn\n",
        "        self.value_optimizer_fn = value_optimizer_fn\n",
        "        self.value_optimizer_lr = value_optimizer_lr\n",
        "        self.max_gradient_norm = max_gradient_norm\n",
        "        self.training_strategy_fn = training_strategy_fn\n",
        "        self.evaluation_strategy_fn = evaluation_strategy_fn\n",
        "        self.n_warmup_batches = n_warmup_batches\n",
        "        self.update_target_every_steps = update_target_every_steps\n",
        "        self.tau = tau\n",
        "\n",
        "    def optimize_model(self, experiences):\n",
        "        idxs, weights, \\\n",
        "        (states, actions, rewards, next_states, is_terminals) = experiences\n",
        "        weights = self.online_model.numpy_float_to_device(weights)\n",
        "        batch_size = len(is_terminals)\n",
        "\n",
        "        argmax_a_q_sp = self.online_model(next_states).max(1)[1]\n",
        "        q_sp = self.target_model(next_states).detach()\n",
        "        max_a_q_sp = q_sp[\n",
        "            np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n",
        "        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n",
        "        q_sa = self.online_model(states).gather(1, actions)\n",
        "\n",
        "        td_error = q_sa - target_q_sa\n",
        "        value_loss = (weights * td_error).pow(2).mul(0.5).mean()\n",
        "        self.value_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.online_model.parameters(),\n",
        "                                       self.max_gradient_norm)\n",
        "        self.value_optimizer.step()\n",
        "\n",
        "        priorities = np.abs(td_error.detach().cpu().numpy())\n",
        "        self.replay_buffer.update(idxs, priorities)\n",
        "\n",
        "    def interaction_step(self, state, env):\n",
        "        action = self.training_strategy.select_action(self.online_model, state)\n",
        "        new_state, reward, is_terminal, info = env.step(action)\n",
        "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
        "        is_failure = is_terminal and not is_truncated\n",
        "        experience = (state, action, reward, new_state, float(is_failure))\n",
        "\n",
        "        self.replay_buffer.store(experience)\n",
        "        self.episode_reward[-1] += reward\n",
        "        self.episode_timestep[-1] += 1\n",
        "        self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n",
        "        return new_state, is_terminal\n",
        "\n",
        "    def update_network(self, tau=None):\n",
        "        tau = self.tau if tau is None else tau\n",
        "        for target, online in zip(self.target_model.parameters(),\n",
        "                                  self.online_model.parameters()):\n",
        "            target_ratio = (1.0 - tau) * target.data\n",
        "            online_ratio = tau * online.data\n",
        "            mixed_weights = target_ratio + online_ratio\n",
        "            target.data.copy_(mixed_weights)\n",
        "\n",
        "    def train(self, make_env_fn, make_env_kargs, seed, gamma,\n",
        "              max_minutes, max_episodes, goal_mean_100_reward):\n",
        "        training_start, last_debug_time = time.time(), float('-inf')\n",
        "\n",
        "        self.checkpoint_dir = tempfile.mkdtemp()\n",
        "        self.make_env_fn = make_env_fn\n",
        "        self.make_env_kargs = make_env_kargs\n",
        "        self.seed = seed\n",
        "        self.gamma = gamma\n",
        "\n",
        "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
        "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
        "\n",
        "        nS, nA = env.observation_space.shape[0], env.action_space.n\n",
        "        self.episode_timestep = []\n",
        "        self.episode_reward = []\n",
        "        self.episode_seconds = []\n",
        "        self.evaluation_scores = []\n",
        "        self.episode_exploration = []\n",
        "\n",
        "        self.target_model = self.value_model_fn(nS, nA)\n",
        "        self.online_model = self.value_model_fn(nS, nA)\n",
        "        self.update_network(tau=1.0)\n",
        "\n",
        "        self.value_optimizer = self.value_optimizer_fn(self.online_model,\n",
        "                                                       self.value_optimizer_lr)\n",
        "\n",
        "        self.replay_buffer = self.replay_buffer_fn()\n",
        "        self.training_strategy = training_strategy_fn()\n",
        "        self.evaluation_strategy = evaluation_strategy_fn()\n",
        "\n",
        "        result = np.empty((max_episodes, 5))\n",
        "        result[:] = np.nan\n",
        "        training_time = 0\n",
        "        for episode in range(1, max_episodes + 1):\n",
        "            episode_start = time.time()\n",
        "\n",
        "            state, is_terminal = env.reset(), False\n",
        "            self.episode_reward.append(0.0)\n",
        "            self.episode_timestep.append(0.0)\n",
        "            self.episode_exploration.append(0.0)\n",
        "\n",
        "            for step in count():\n",
        "                state, is_terminal = self.interaction_step(state, env)\n",
        "\n",
        "                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
        "                if len(self.replay_buffer) > min_samples:\n",
        "                    experiences = self.replay_buffer.sample()\n",
        "                    idxs, weights, samples = experiences\n",
        "                    experiences = self.online_model.load(samples)\n",
        "                    experiences = (idxs, weights) + (experiences,)\n",
        "                    self.optimize_model(experiences)\n",
        "\n",
        "                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n",
        "                    self.update_network()\n",
        "\n",
        "                if is_terminal:\n",
        "                    gc.collect()\n",
        "                    break\n",
        "\n",
        "            # stats\n",
        "            episode_elapsed = time.time() - episode_start\n",
        "            self.episode_seconds.append(episode_elapsed)\n",
        "            training_time += episode_elapsed\n",
        "            evaluation_score, _ = self.evaluate(self.online_model, env)\n",
        "            self.save_checkpoint(episode-1, self.online_model)\n",
        "\n",
        "            total_step = int(np.sum(self.episode_timestep))\n",
        "            self.evaluation_scores.append(evaluation_score)\n",
        "\n",
        "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
        "            std_10_reward = np.std(self.episode_reward[-10:])\n",
        "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
        "            std_100_reward = np.std(self.episode_reward[-100:])\n",
        "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
        "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
        "            lst_100_exp_rat = np.array(\n",
        "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
        "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
        "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
        "\n",
        "            wallclock_elapsed = time.time() - training_start\n",
        "            result[episode-1] = total_step, mean_100_reward, \\\n",
        "                mean_100_eval_score, training_time, wallclock_elapsed\n",
        "\n",
        "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
        "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
        "            reached_max_episodes = episode >= max_episodes\n",
        "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
        "            training_is_over = reached_max_minutes or \\\n",
        "                               reached_max_episodes or \\\n",
        "                               reached_goal_mean_reward\n",
        "\n",
        "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
        "            debug_message = 'el {}, ep {:04}, ts {:06}, '\n",
        "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
        "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
        "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
        "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
        "            debug_message = debug_message.format(\n",
        "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward,\n",
        "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
        "                mean_100_eval_score, std_100_eval_score)\n",
        "            print(debug_message, end='\\r', flush=True)\n",
        "            if reached_debug_time or training_is_over:\n",
        "                print(ERASE_LINE + debug_message, flush=True)\n",
        "                last_debug_time = time.time()\n",
        "            if training_is_over:\n",
        "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
        "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
        "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
        "                break\n",
        "\n",
        "        final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n",
        "        wallclock_time = time.time() - training_start\n",
        "        print('Training complete.')\n",
        "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
        "              ' {:.2f}s wall-clock time.\\n'.format(\n",
        "                  final_eval_score, score_std, training_time, wallclock_time))\n",
        "        env.close() ; del env\n",
        "        self.get_cleaned_checkpoints()\n",
        "        return result, final_eval_score, training_time, wallclock_time\n",
        "\n",
        "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
        "        rs = []\n",
        "        for _ in range(n_episodes):\n",
        "            s, d = eval_env.reset(), False\n",
        "            rs.append(0)\n",
        "            for _ in count():\n",
        "                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n",
        "                s, r, d, _ = eval_env.step(a)\n",
        "                rs[-1] += r\n",
        "                if d: break\n",
        "        return np.mean(rs), np.std(rs)\n",
        "\n",
        "    def get_cleaned_checkpoints(self, n_checkpoints=5):\n",
        "        try:\n",
        "            return self.checkpoint_paths\n",
        "        except AttributeError:\n",
        "            self.checkpoint_paths = {}\n",
        "\n",
        "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
        "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
        "        last_ep = max(paths_dic.keys())\n",
        "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
        "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=int)-1\n",
        "\n",
        "        for idx, path in paths_dic.items():\n",
        "            if idx in checkpoint_idxs:\n",
        "                self.checkpoint_paths[idx] = path\n",
        "            else:\n",
        "                os.unlink(path)\n",
        "\n",
        "        return self.checkpoint_paths\n",
        "\n",
        "    def demo_last_legacy(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n",
        "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
        "\n",
        "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
        "        last_ep = max(checkpoint_paths.keys())\n",
        "        self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
        "\n",
        "        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n",
        "        env.close()\n",
        "        data = get_gif_html(env_videos=env.videos,\n",
        "                            title=title.format(self.__class__.__name__),\n",
        "                            max_n_videos=max_n_videos)\n",
        "        del env\n",
        "        return HTML(data=data)\n",
        "\n",
        "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n",
        "        # Create a temporary directory for video storage\n",
        "        temp_dir = os.path.join(os.getcwd(), 'temp_videos')\n",
        "        if not os.path.exists(temp_dir):\n",
        "            os.makedirs(temp_dir)\n",
        "\n",
        "        # Make environment with video recording\n",
        "        env = self.make_env_fn(**self.make_env_kargs,\n",
        "                              monitor_mode='evaluation',\n",
        "                              render=True)\n",
        "        env = RecordVideo(env, temp_dir, episode_trigger=lambda x: True)  # Record all episodes\n",
        "        env = RecordEpisodeStatistics(env)  # To capture episode stats\n",
        "\n",
        "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
        "        last_ep = max(checkpoint_paths.keys())\n",
        "        self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
        "\n",
        "        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n",
        "        env.close()\n",
        "\n",
        "        # Gather video and metadata files\n",
        "        videos = []\n",
        "        for video_file in os.listdir(temp_dir):\n",
        "            if video_file.endswith('.mp4'):\n",
        "                video_path = os.path.join(temp_dir, video_file)\n",
        "                meta_path = os.path.join(temp_dir, os.path.splitext(video_file)[0] + '.json')\n",
        "                videos.append((video_path, meta_path))\n",
        "\n",
        "        # Generate GIF HTML\n",
        "        data = get_gif_html(env_videos=videos,\n",
        "                            title=title.format(self.__class__.__name__),\n",
        "                            max_n_videos=max_n_videos)\n",
        "\n",
        "        # Clean up temporary directory\n",
        "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "\n",
        "        del env\n",
        "        return HTML(data=data)\n",
        "\n",
        "    def demo_progression_legacy(self, title='{} Agent progression', max_n_videos=5):\n",
        "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
        "\n",
        "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
        "        for i in sorted(checkpoint_paths.keys()):\n",
        "            self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
        "            self.evaluate(self.online_model, env, n_episodes=1)\n",
        "\n",
        "        env.close()\n",
        "        data = get_gif_html(env_videos=env.videos,\n",
        "                            title=title.format(self.__class__.__name__),\n",
        "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
        "                            max_n_videos=max_n_videos)\n",
        "        del env\n",
        "        return HTML(data=data)\n",
        "\n",
        "    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n",
        "        # Create a temporary directory for video storage\n",
        "        temp_dir = os.path.join(os.getcwd(), 'temp_videos_progression')\n",
        "        if not os.path.exists(temp_dir):\n",
        "            os.makedirs(temp_dir)\n",
        "\n",
        "        # Make environment with video recording\n",
        "        env = self.make_env_fn(**self.make_env_kargs,\n",
        "                              monitor_mode='evaluation',\n",
        "                              render=True)\n",
        "        env = RecordVideo(env, temp_dir, episode_trigger=lambda x: True)  # Record all episodes\n",
        "        env = RecordEpisodeStatistics(env)  # To capture episode stats\n",
        "\n",
        "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
        "        for i in sorted(checkpoint_paths.keys()):\n",
        "            self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
        "            self.evaluate(self.online_model, env, n_episodes=1)\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        # Gather video and metadata files\n",
        "        videos = []\n",
        "        for video_file in sorted(os.listdir(temp_dir)):\n",
        "            if video_file.endswith('.mp4'):\n",
        "                video_path = os.path.join(temp_dir, video_file)\n",
        "                meta_path = os.path.join(temp_dir, os.path.splitext(video_file)[0] + '.json')\n",
        "                videos.append((video_path, meta_path))\n",
        "\n",
        "        # Generate GIF HTML\n",
        "        data = get_gif_html(env_videos=videos,\n",
        "                            title=title.format(self.__class__.__name__),\n",
        "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
        "                            max_n_videos=max_n_videos)\n",
        "\n",
        "        # Clean up temporary directory\n",
        "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
        "\n",
        "        del env\n",
        "        return HTML(data=data)\n",
        "\n",
        "    def save_checkpoint(self, episode_idx, model):\n",
        "        torch.save(model.state_dict(),\n",
        "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "-fSrfdHpZNGc"
      },
      "outputs": [],
      "source": [
        "per_results = []\n",
        "best_agent, best_eval_score = None, float('-inf')\n",
        "for seed in SEEDS:\n",
        "    environment_settings = {\n",
        "        'env_name': 'CartPole-v1',\n",
        "        'gamma': 1.00,\n",
        "        'max_minutes': 30,\n",
        "        'max_episodes': 10000,\n",
        "        'goal_mean_100_reward': 475\n",
        "    }\n",
        "\n",
        "    value_model_fn = lambda nS, nA: FCDuelingQ(nS, nA, hidden_dims=(512,128))\n",
        "    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n",
        "    value_optimizer_lr = 0.0001\n",
        "    max_gradient_norm = float('inf')\n",
        "\n",
        "    training_strategy_fn = lambda: EGreedyExpStrategy(init_epsilon=1.0,\n",
        "                                                      min_epsilon=0.3,\n",
        "                                                      decay_steps=20000)\n",
        "    evaluation_strategy_fn = lambda: GreedyStrategy()\n",
        "\n",
        "    # replay_buffer_fn = lambda: ReplayBuffer(max_size=10000, batch_size=64)\n",
        "    # replay_buffer_fn = lambda: PrioritizedReplayBuffer(\n",
        "    #     max_samples=10000, batch_size=64, rank_based=True,\n",
        "    #     alpha=0.6, beta0=0.1, beta_rate=0.99995)\n",
        "    replay_buffer_fn = lambda: PrioritizedReplayBuffer(\n",
        "        max_samples=20000, batch_size=64, rank_based=False,\n",
        "        alpha=0.6, beta0=0.1, beta_rate=0.99995)\n",
        "    n_warmup_batches = 5\n",
        "    update_target_every_steps = 1\n",
        "    tau = 0.01\n",
        "\n",
        "    env_name, gamma, max_minutes, \\\n",
        "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
        "    agent = PER(replay_buffer_fn,\n",
        "                value_model_fn,\n",
        "                value_optimizer_fn,\n",
        "                value_optimizer_lr,\n",
        "                max_gradient_norm,\n",
        "                training_strategy_fn,\n",
        "                evaluation_strategy_fn,\n",
        "                n_warmup_batches,\n",
        "                update_target_every_steps,\n",
        "                tau)\n",
        "\n",
        "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
        "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
        "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
        "    per_results.append(result)\n",
        "    if final_eval_score > best_eval_score:\n",
        "        best_eval_score = final_eval_score\n",
        "        best_agent = agent\n",
        "per_results = np.array(per_results)\n",
        "_ = BEEP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "2dXV6JyhZNGc"
      },
      "outputs": [],
      "source": [
        "best_agent.demo_progression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg1AiaOUZNGc"
      },
      "outputs": [],
      "source": [
        "best_agent.demo_last()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFgwLzP8ZNGc"
      },
      "outputs": [],
      "source": [
        "per_max_t, per_max_r, per_max_s, per_max_sec, per_max_rt = np.max(per_results, axis=0).T\n",
        "per_min_t, per_min_r, per_min_s, per_min_sec, per_min_rt = np.min(per_results, axis=0).T\n",
        "per_mean_t, per_mean_r, per_mean_s, per_mean_sec, per_mean_rt = np.mean(per_results, axis=0).T\n",
        "per_x = np.arange(np.max(\n",
        "    (len(per_mean_s), len(dueling_ddqn_mean_s))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ22zIB9ZNGc"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5, 1, figsize=(15,30), sharey=False, sharex=True)\n",
        "\n",
        "# Dueling DDQN\n",
        "axs[0].plot(dueling_ddqn_max_r, 'r', linewidth=1)\n",
        "axs[0].plot(dueling_ddqn_min_r, 'r', linewidth=1)\n",
        "axs[0].plot(dueling_ddqn_mean_r, 'r:', label='Dueling DDQN', linewidth=2)\n",
        "axs[0].fill_between(\n",
        "    dueling_ddqn_x, dueling_ddqn_min_r, dueling_ddqn_max_r, facecolor='r', alpha=0.3)\n",
        "\n",
        "axs[1].plot(dueling_ddqn_max_s, 'r', linewidth=1)\n",
        "axs[1].plot(dueling_ddqn_min_s, 'r', linewidth=1)\n",
        "axs[1].plot(dueling_ddqn_mean_s, 'r:', label='Dueling DDQN', linewidth=2)\n",
        "axs[1].fill_between(\n",
        "    dueling_ddqn_x, dueling_ddqn_min_s, dueling_ddqn_max_s, facecolor='r', alpha=0.3)\n",
        "\n",
        "axs[2].plot(dueling_ddqn_max_t, 'r', linewidth=1)\n",
        "axs[2].plot(dueling_ddqn_min_t, 'r', linewidth=1)\n",
        "axs[2].plot(dueling_ddqn_mean_t, 'r:', label='Dueling DDQN', linewidth=2)\n",
        "axs[2].fill_between(\n",
        "    dueling_ddqn_x, dueling_ddqn_min_t, dueling_ddqn_max_t, facecolor='r', alpha=0.3)\n",
        "\n",
        "axs[3].plot(dueling_ddqn_max_sec, 'r', linewidth=1)\n",
        "axs[3].plot(dueling_ddqn_min_sec, 'r', linewidth=1)\n",
        "axs[3].plot(dueling_ddqn_mean_sec, 'r:', label='Dueling DDQN', linewidth=2)\n",
        "axs[3].fill_between(\n",
        "    dueling_ddqn_x, dueling_ddqn_min_sec, dueling_ddqn_max_sec, facecolor='r', alpha=0.3)\n",
        "\n",
        "axs[4].plot(dueling_ddqn_max_rt, 'r', linewidth=1)\n",
        "axs[4].plot(dueling_ddqn_min_rt, 'r', linewidth=1)\n",
        "axs[4].plot(dueling_ddqn_mean_rt, 'r:', label='Dueling DDQN', linewidth=2)\n",
        "axs[4].fill_between(\n",
        "    dueling_ddqn_x, dueling_ddqn_min_rt, dueling_ddqn_max_rt, facecolor='r', alpha=0.3)\n",
        "\n",
        "# PER\n",
        "axs[0].plot(per_max_r, 'k', linewidth=1)\n",
        "axs[0].plot(per_min_r, 'k', linewidth=1)\n",
        "axs[0].plot(per_mean_r, 'k', label='PER', linewidth=2)\n",
        "axs[0].fill_between(per_x, per_min_r, per_max_r, facecolor='k', alpha=0.3)\n",
        "\n",
        "axs[1].plot(per_max_s, 'k', linewidth=1)\n",
        "axs[1].plot(per_min_s, 'k', linewidth=1)\n",
        "axs[1].plot(per_mean_s, 'k', label='PER', linewidth=2)\n",
        "axs[1].fill_between(per_x, per_min_s, per_max_s, facecolor='k', alpha=0.3)\n",
        "\n",
        "axs[2].plot(per_max_t, 'k', linewidth=1)\n",
        "axs[2].plot(per_min_t, 'k', linewidth=1)\n",
        "axs[2].plot(per_mean_t, 'k', label='PER', linewidth=2)\n",
        "axs[2].fill_between(per_x, per_min_t, per_max_t, facecolor='k', alpha=0.3)\n",
        "\n",
        "axs[3].plot(per_max_sec, 'k', linewidth=1)\n",
        "axs[3].plot(per_min_sec, 'k', linewidth=1)\n",
        "axs[3].plot(per_mean_sec, 'k', label='PER', linewidth=2)\n",
        "axs[3].fill_between(per_x, per_min_sec, per_max_sec, facecolor='k', alpha=0.3)\n",
        "\n",
        "axs[4].plot(per_max_rt, 'k', linewidth=1)\n",
        "axs[4].plot(per_min_rt, 'k', linewidth=1)\n",
        "axs[4].plot(per_mean_rt, 'k', label='PER', linewidth=2)\n",
        "axs[4].fill_between(per_x, per_min_rt, per_max_rt, facecolor='k', alpha=0.3)\n",
        "\n",
        "# ALL\n",
        "axs[0].set_title('Moving Avg Reward (Training)')\n",
        "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
        "axs[2].set_title('Total Steps')\n",
        "axs[3].set_title('Training Time')\n",
        "axs[4].set_title('Wall-clock Time')\n",
        "plt.xlabel('Episodes')\n",
        "axs[0].legend(loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWw8ss9kZNGd"
      },
      "outputs": [],
      "source": [
        "per_root_dir = os.path.join(RESULTS_DIR, 'per')\n",
        "not os.path.exists(per_root_dir) and os.makedirs(per_root_dir)\n",
        "\n",
        "np.save(os.path.join(per_root_dir, 'x'), per_x)\n",
        "\n",
        "np.save(os.path.join(per_root_dir, 'max_r'), per_max_r)\n",
        "np.save(os.path.join(per_root_dir, 'min_r'), per_min_r)\n",
        "np.save(os.path.join(per_root_dir, 'mean_r'), per_mean_r)\n",
        "\n",
        "np.save(os.path.join(per_root_dir, 'max_s'), per_max_s)\n",
        "np.save(os.path.join(per_root_dir, 'min_s'), per_min_s )\n",
        "np.save(os.path.join(per_root_dir, 'mean_s'), per_mean_s)\n",
        "\n",
        "np.save(os.path.join(per_root_dir, 'max_t'), per_max_t)\n",
        "np.save(os.path.join(per_root_dir, 'min_t'), per_min_t)\n",
        "np.save(os.path.join(per_root_dir, 'mean_t'), per_mean_t)\n",
        "\n",
        "np.save(os.path.join(per_root_dir, 'max_sec'), per_max_sec)\n",
        "np.save(os.path.join(per_root_dir, 'min_sec'), per_min_sec)\n",
        "np.save(os.path.join(per_root_dir, 'mean_sec'), per_mean_sec)\n",
        "\n",
        "np.save(os.path.join(per_root_dir, 'max_rt'), per_max_rt)\n",
        "np.save(os.path.join(per_root_dir, 'min_rt'), per_min_rt)\n",
        "np.save(os.path.join(per_root_dir, 'mean_rt'), per_mean_rt)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}